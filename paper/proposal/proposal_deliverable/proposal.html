<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>SENG-404 Proposal: Prompt Engineering and Project Engagement in LLM-Enabled Repositories</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="proposal_files/libs/clipboard/clipboard.min.js"></script>
<script src="proposal_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="proposal_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="proposal_files/libs/quarto-html/popper.min.js"></script>
<script src="proposal_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="proposal_files/libs/quarto-html/anchor.min.js"></script>
<link href="proposal_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="proposal_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="proposal_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="proposal_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="proposal_files/libs/bootstrap/bootstrap-41d128fa7d2fc35e15c5440c3becc28b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">SENG-404 Proposal: Prompt Engineering and Project Engagement in LLM-Enabled Repositories</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Evan Strasdin - V00907185<br>
<a href="https://github.com/n4m3name/SENG404-Project">Git Link</a></p>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Large Language Models (LLMs) like GPT-4 are now central to many software systems, with open-source tools such as LangChain, AutoGPT, and llama.cpp enabling developers to build entire applications around model behavior. In these systems, prompts are not just inputs: they function as logic, guiding how the model reasons, responds, and performs tasks. As a result, prompt engineering has become a core software development activity.</p>
<p>Recent research has begun to formalize this space. Studies have introduced taxonomies of prompt types, such as zero-shot, few-shot, and chain-of-thought prompting <a href="#references">(Zhou et al., 2024)</a>, and proposed metrics for evaluating prompt quality, including reasoning coherence <a href="#references">(Li et al., 2023)</a> and cost-efficiency <a href="#references">(Diao et al., 2024)</a>. Other work has analyzed how prompts evolve over time in open-source repositories <a href="#references">(Tafreshipour et al., 2024)</a>.</p>
<p>However, it remains unclear whether prompt characteristics, both in terms of type and engineering behaviors (such as complexity, reuse, or collaborative authorship), are associated with broader indicators of project activity and engagement, such as contributor count, issue resolution rate, or project growth.</p>
<p>This study addresses that gap by combining prompt classification with software repository mining. We will apply established prompt type categories and measure engineering-related characteristics such as length, example count, and contributor overlap. Where appropriate, we may explore additional quality metrics inspired by recent research. Our goal is to uncover whether observable prompt patterns relate to collaboration and development dynamics in real-world LLM-integrated projects.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
<section id="research-question" class="level2">
<h2 class="anchored" data-anchor-id="research-question">Research Question</h2>
<p>Do prompt characteristics, such as type, complexity, reuse, and collaborative authorship, correlate with indicators of project activity and engagement in open-source LLM-integrated repositories?</p>
<p>We will analyze prompt artifacts extracted from public repositories and apply a combination of classification (for example, by prompt type) and metric-based evaluation (such as prompt length and contributor interaction). These features will be compared against repository-level signals of activity, including contributor count, issue resolution rate, and project growth over time.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
<section id="expected-data-sources" class="level2">
<h2 class="anchored" data-anchor-id="expected-data-sources">Expected Data Sources</h2>
<section id="repository-selection-github-search-and-github-api" class="level3">
<h3 class="anchored" data-anchor-id="repository-selection-github-search-and-github-api">Repository Selection (GitHub Search and GitHub API)</h3>
<p>To identify candidate repositories for our study, we performed a keyword-based search over public GitHub code using the <a href="https://docs.github.com/en/rest/search?apiVersion=2022-11-28">GitHub Search API</a>. We designed a list of 30+ LLM-related keywords targeting common APIs, prompt engineering patterns, and framework imports, such as:</p>
<ul>
<li><code>openai.ChatCompletion.create</code>, <code>PromptTemplate</code>, <code>RetrievalQA</code></li>
<li><code>"prompt = "</code>, <code>"system": "</code></li>
<li><code>llama_cpp</code>, <code>AutoGPT</code>, <code>FAISS</code>, <code>transformers.pipeline</code>, and others</li>
</ul>
<p>For each keyword, we retrieved up to 250 results across 5 paginated requests (50 results per page). Each code match was associated with metadata including the repository name, file path, and matched keyword.</p>
<p>To rank repository relevance, we grouped results by repository and counted how many distinct keywords each repository matched. The rationale is that repositories containing multiple LLM-specific keywords are more likely to integrate structured prompt engineering practices rather than incidental mentions.</p>
<p>The output is saved in both <code>.csv</code> and <code>.json</code> formats under the <code>/data</code> directory:</p>
<ul>
<li><a href="../data/matched_repositories.csv">matched_repositories.csv</a></li>
<li><a href="../data/matched_repositories.json">matched_repositories.json</a></li>
</ul>
<p>The complete code for this data collection and early processing process is documented in <a href="../src/data_sourcing.ipynb">data_sourcing.ipynb</a>.</p>
</section>
<section id="project-metadata-github-api-and-ghtorrent" class="level3">
<h3 class="anchored" data-anchor-id="project-metadata-github-api-and-ghtorrent">Project Metadata (GitHub API and GHTorrent)</h3>
<p>To supplement our keyword-based prompt discovery with repository-level context, we retrieved metadata for each matched repository using the GitHub API.</p>
<p>For each repository, we collected:</p>
<ul>
<li><strong>Stars, forks, watchers</strong> — as high-level proxies for community interest</li>
<li><strong>Open issues</strong> — to approximate project activity and maintenance</li>
<li><strong>Repository timestamps</strong> (<code>created_at</code>, <code>updated_at</code>, <code>pushed_at</code>) — for lifecycle analysis</li>
<li><strong>License, language, and description</strong> — to support filtering and categorization</li>
<li><strong>Default branch</strong> — required for linking to prompt files and future mining</li>
</ul>
<p>This metadata was merged with our keyword-matching dataset (see <a href="../data/combined_repository_data.csv">combined_repository_data.csv</a>), creating a unified view of prompt relevance and project health. The merged data is saved in both <code>.csv</code> and <code>.json</code> format to support both manual review and downstream analysis.</p>
<p>If time and scope allow, we may also incorporate contributor-level metrics (e.g.&nbsp;commit count, contributor overlap) and validate repository activity using <a href="http://ghtorrent.org/">GHTorrent</a> to support scalable analysis beyond API rate limits.</p>
</section>
<section id="prompt-artifacts" class="level3">
<h3 class="anchored" data-anchor-id="prompt-artifacts">Prompt Artifacts</h3>
<p>Prompt strings will be extracted from relevant source files using a combination of static code parsing (e.g.&nbsp;Python AST), regex-based string analysis, and heuristics for identifying natural language input patterns. Prompts will be categorized by type using existing taxonomies <a href="#references">(Zhou et al., 2024)</a>, and features such as length, example count, and reuse will be computed automatically. File-level and commit-level metadata will be retained for all prompt artifacts.</p>
</section>
<section id="optional-software-heritage-graph" class="level3">
<h3 class="anchored" data-anchor-id="optional-software-heritage-graph">Optional: Software Heritage Graph</h3>
<p>If time and tooling permit, the Software Heritage Graph will be used to trace prompt files across forks and versions, allowing us to observe reuse and evolution beyond a single repository.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
</section>
<section id="research-strategy" class="level2">
<h2 class="anchored" data-anchor-id="research-strategy">Research Strategy</h2>
<section id="tools-and-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="tools-and-dependencies">Tools and Dependencies</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 42%">
<col style="width: 8%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Tool / Framework</th>
<th>Purpose</th>
<th>Familiarity</th>
<th>Cost / Access</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>GitHub API</strong></td>
<td>Repository metadata and file access</td>
<td>High</td>
<td>Free (rate-limited)</td>
</tr>
<tr class="even">
<td><strong>Jupyter + Python stack</strong></td>
<td>Data processing and analysis (<code>pandas</code>, <code>seaborn</code>, etc.)</td>
<td>High</td>
<td>Free</td>
</tr>
<tr class="odd">
<td><strong>Quarto</strong></td>
<td>Reproducible reports and publication output (PDF, HTML)</td>
<td>High</td>
<td>Free</td>
</tr>
<tr class="even">
<td><strong>LaTeX</strong></td>
<td>Typesetting for the written report and PDF export</td>
<td>High</td>
<td>Free (via Overleaf or local TeX)</td>
</tr>
<tr class="odd">
<td><strong>R</strong></td>
<td>Statistical modeling or specialized visualization</td>
<td>Moderate</td>
<td>Free</td>
</tr>
<tr class="even">
<td><strong>GitHub Copilot Chat</strong></td>
<td>Code completion, analysis automation</td>
<td>High</td>
<td>$10/month (covered by user)</td>
</tr>
<tr class="odd">
<td><strong>GHTorrent (optional)</strong></td>
<td>Supplemental repo metadata at scale</td>
<td>Low</td>
<td>Free</td>
</tr>
<tr class="even">
<td><strong>Software Heritage (optional)</strong></td>
<td>Track prompt reuse and code lineage</td>
<td>Low</td>
<td>Free</td>
</tr>
</tbody>
</table>
</section>
<section id="preliminary-validation-pilot-analysis" class="level3">
<h3 class="anchored" data-anchor-id="preliminary-validation-pilot-analysis">Preliminary Validation: Pilot Analysis</h3>
<p>To assess the reliability and relevance of our keyword-based repository selection, we conducted a series of exploratory analyses on the merged dataset, described in <a href="https://github.com/n4m3name/SENG404-Project/blob/master/src/pilot/pilot.pdf">../../src/pilot/pilot.qmd</a>. These pilot experiments served to validate the feasibility of our research design and guide the selection of downstream analysis metrics.</p>
<section id="key-findings" class="level4">
<h4 class="anchored" data-anchor-id="key-findings">Key Findings:</h4>
<ul>
<li><strong>Keyword Match Signal</strong>: While a majority of repositories matched only a single LLM-related keyword, a substantial number exhibited 2+ matches—suggesting deeper integration of prompt-related logic. We use keyword count as a proxy for LLM integration intensity.</li>
<li><strong>Engagement Metrics</strong>: Repository engagement (stars, forks, watchers) showed wide variance across the sample, and log-transformed engagement distributions were suitable for analysis. This supports the use of these metrics as comparative signals for relevance and community interest.</li>
<li><strong>Weak Correlation Observed</strong>: A preliminary analysis found a weak positive correlation between the number of LLM-related keywords and a composite engagement score (Pearson’s r ≈ 0.07). This suggests that while there may not be a linear relationship, deeper patterns could emerge with prompt-level analysis.</li>
<li><strong>Temporal Coverage</strong>: The majority of repositories were created after 2020, with a sharp increase in 2023–2024, coinciding with the surge in public interest in LLMs. This confirms that our sample reflects contemporary LLM development practices.</li>
<li><strong>Keyword Patterns</strong>: Certain keywords—such as <code>"prompt="</code>, <code>"system"</code>, <code>ChatOpenAI</code>, and <code>PromptTemplate</code>—appeared with high frequency, and some showed stronger presence in high-engagement repositories. These patterns can inform our prompt classification strategy.</li>
</ul>
<p>These initial results confirm that our keyword-matching and metadata pipeline successfully surfaces a diverse set of LLM-relevant repositories, suitable for further analysis. The pilot also helped us calibrate the importance of each metadata feature, refine our engagement scoring strategy, and identify prompts for extraction.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
</section>
</section>
<section id="methodology-and-timeline" class="level2">
<h2 class="anchored" data-anchor-id="methodology-and-timeline">Methodology and Timeline</h2>
<div id="d6668bb7" class="cell" data-message="false" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="proposal_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="project-task-breakdown-and-timeline-rationale" class="level3">
<h3 class="anchored" data-anchor-id="project-task-breakdown-and-timeline-rationale">Project Task Breakdown and Timeline Rationale</h3>
<p>This timeline outlines the major components of the project, from early analysis to final presentation. Each task is scheduled based on its dependencies and estimated effort, with built-in buffer time to accommodate review and refinement.</p>
<section id="data-cleaning-july-1721" class="level4">
<h4 class="anchored" data-anchor-id="data-cleaning-july-1721">1. Data Cleaning (July 17–21)</h4>
<p>While I completed the core data collection earlier (GitHub keyword scraping and repository metadata extraction), this stage involves sanitizing the dataset: Handling missing values, normalizing timestamp formats, and verifying repository uniqueness. I will likely attempt to augment the data <a href="#project-metadata-(github-api-and-ghtorrent)">as discussed</a>.<br>
Estimated time: ~5 days to ensure a clean, reliable dataset before analysis.</p>
</section>
<section id="pilot-analysis-writeup-july-1724" class="level4">
<h4 class="anchored" data-anchor-id="pilot-analysis-writeup-july-1724">2. Pilot Analysis + Writeup (July 17–24)</h4>
<p>I began exploratory analysis on July 17 to validate that my keyword-matching strategy effectively surfaces LLM-relevant repositories. This includes visualizing keyword density, engagement metrics, and identifying early patterns. I’m also producing a formal writeup for inclusion in the interim report.<br>
Estimated time: ~1 week to complete analysis and documentation.</p>
</section>
<section id="interim-report-july-22august-1" class="level4">
<h4 class="anchored" data-anchor-id="interim-report-july-22august-1">3. Interim Report (July 22–August 1)</h4>
<p>The interim report will summarize my progress so far, justify my approach, and include new findings as needed. It will also outline new tools, feasibility, and methodology moving forward.<br>
Estimated time: ~10 days to allow for drafting, editing, and submission.</p>
</section>
<section id="prompt-extraction-july-25august-5" class="level4">
<h4 class="anchored" data-anchor-id="prompt-extraction-july-25august-5">4. Prompt Extraction (July 25–August 5)</h4>
<p>I will extract actual prompt strings from the matched files using static code parsing, regex, and heuristics. This step is critical, as it forms the foundation for all downstream prompt classification and metric calculations.<br>
Estimated time: ~10 days for implementation, testing, and validation.</p>
</section>
<section id="prompt-classification-august-110" class="level4">
<h4 class="anchored" data-anchor-id="prompt-classification-august-110">5. Prompt Classification (August 1–10)</h4>
<p>Once prompts are extracted, I’ll classify them according to prompt engineering taxonomies (e.g., zero-shot, few-shot, chain-of-thought) and compute structural features such as length, number of examples, or use of system roles.<br>
Estimated time: ~10 days, with some overlap with extraction to save time.</p>
</section>
<section id="engagement-modeling-august-615" class="level4">
<h4 class="anchored" data-anchor-id="engagement-modeling-august-615">6. Engagement Modeling (August 6–15)</h4>
<p>Here I’ll test whether different prompt characteristics are associated with engagement metrics (stars, forks, contributors, etc.). This will involve statistical modeling and visualization.<br>
Estimated time: ~10 days for robust experimentation and interpretation.</p>
</section>
<section id="final-report-august-1023" class="level4">
<h4 class="anchored" data-anchor-id="final-report-august-1023">7. Final Report (August 10–23)</h4>
<p>This deliverable includes full documentation of the research question, data, methods, and results. It will be written in LaTeX (or Quarto?) with formal citations and polished visuals.<br>
Estimated time: ~2 weeks, allowing time for writing, feedback, and formatting.</p>
</section>
<section id="presentation-prep-august-1520" class="level4">
<h4 class="anchored" data-anchor-id="presentation-prep-august-1520">8. Presentation Prep (August 15–20)</h4>
<p>I’ll develop slides, speaking notes, and visuals to clearly present my findings.<br>
Estimated time: ~5 days for design and rehearsal.</p>
<p>Each task was scheduled to overlap with minimal dependencies while leaving room for unforeseen revisions. Emphasis was placed on frontloading the classification and modeling work to ensure the final report and presentation are built on a stable foundation of results.</p>
</section>
</section>
<section id="workflow-specification" class="level3">
<h3 class="anchored" data-anchor-id="workflow-specification">Workflow Specification</h3>
<p>This section outlines the planned end-to-end methodology for the project, from data acquisition to statistical analysis and presentation.</p>
<section id="data-sources" class="level4">
<h4 class="anchored" data-anchor-id="data-sources">Data Sources</h4>
<p>The study uses two layers of data:</p>
<ul>
<li><strong>Keyword-Matched Repositories</strong> from the GitHub Search API, filtered using a curated set of 30+ LLM-related keywords.</li>
<li><strong>Repository Metadata</strong> from the GitHub REST API, including stars, forks, watchers, open issues, and timestamps.</li>
</ul>
<p>Where feasible, supplemental metadata may be obtained from <strong>GHTorrent</strong> or <strong>Software Heritage</strong> to validate or extend API-based insights (especially for contributor history or prompt reuse across forks).</p>
</section>
<section id="filtering-criteria" class="level4">
<h4 class="anchored" data-anchor-id="filtering-criteria">Filtering Criteria</h4>
<ul>
<li><strong>Keyword Count ≥ 2</strong>: Repositories with at least two distinct LLM-related keyword matches will be prioritized for downstream analysis, as they are more likely to meaningfully integrate prompt engineering.</li>
<li><strong>Language Filter</strong>: Initially limited to Python-based repositories (based on GitHub’s metadata), given tooling and LLM ecosystem alignment.</li>
<li><strong>Repository Status</strong>: Archived or empty repositories are excluded.</li>
</ul>
</section>
<section id="prompt-extraction-and-processing" class="level4">
<h4 class="anchored" data-anchor-id="prompt-extraction-and-processing">Prompt Extraction and Processing</h4>
<ul>
<li><strong>Prompt Detection</strong>: Static code parsing (AST), regex pattern matching, and structural heuristics will be used to extract prompt-like strings.</li>
<li><strong>Artifact Metadata</strong>: Each prompt will retain references to its source file, position, commit ID, and author (if available).</li>
<li><strong>Deduplication and Grouping</strong>: Near-duplicate prompts will be clustered to identify reuse and evolution across files or repositories.</li>
</ul>
</section>
<section id="prompt-classification" class="level4">
<h4 class="anchored" data-anchor-id="prompt-classification">Prompt Classification</h4>
<ul>
<li>Prompts will be categorized using published prompt taxonomies:
<ul>
<li><strong>Zero-shot</strong>, <strong>Few-shot</strong>, <strong>Chain-of-thought</strong>, etc.</li>
</ul></li>
<li>Structural metrics will also be computed:
<ul>
<li>Length (tokens/chars), number of examples, presence of system roles, etc.</li>
</ul></li>
<li>Where feasible, quality-related features from the literature may be computed (e.g., instruction clarity, verbosity, etc.).</li>
</ul>
</section>
<section id="repository-level-metrics" class="level4">
<h4 class="anchored" data-anchor-id="repository-level-metrics">Repository-Level Metrics</h4>
<p>The following indicators will be computed per repository:</p>
<ul>
<li>Contributor count (if feasible)</li>
<li>Engagement score (based on stars, forks, watchers — potentially log-normalized)</li>
<li>Project longevity (based on creation vs last commit)</li>
<li>Issue activity (if GHTorrent integration is successful)</li>
</ul>
</section>
<section id="analysis-and-validation" class="level4">
<h4 class="anchored" data-anchor-id="analysis-and-validation">Analysis and Validation</h4>
<ul>
<li>Exploratory statistics and correlation matrices will be used to assess relationships between prompt-level features and engagement/activity metrics.</li>
<li>Regression models (linear or logistic, depending on outcomes) may be used to model engagement predictors.</li>
<li>If prompt authorship data is extractable, interaction analysis may be run to test whether prompt collaboration predicts broader project activity.</li>
</ul>
</section>
<section id="timeline-consideration" class="level4">
<h4 class="anchored" data-anchor-id="timeline-consideration">Timeline Consideration</h4>
<p>Even though much of the raw data is already acquired, <strong>writing reliable extraction and classification logic</strong>, <strong>validating features</strong>, and <strong>testing models</strong> is time-intensive. The planned timeline includes several overlapping stages and buffer zones to accommodate iteration and debugging.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
</section>
</section>
<section id="expected-results" class="level2">
<h2 class="anchored" data-anchor-id="expected-results">Expected Results</h2>
<p>Based on the preliminary pilot analysis and prior literature, I expect to find:</p>
<ul>
<li><p><strong>Stronger correlations at the prompt level</strong>: While pilot results showed only a weak correlation (r ≈ 0.07) between keyword count and engagement, more meaningful associations may emerge once detailed prompt-level features (e.g.&nbsp;structure, examples, contributor overlap) are extracted and classified.</p></li>
<li><p><strong>Prompt complexity as an engagement signal</strong>: More advanced prompt types—such as few-shot or chain-of-thought—are expected to appear more often in high-engagement repositories, where LLMs are likely used in core logic.</p></li>
<li><p><strong>Collaborative prompt authorship</strong>: Repositories with prompts that involve multiple contributors or edits may correlate with higher contributor counts and a more active issue tracker, suggesting prompt engineering as a shared, iterative process.</p></li>
<li><p><strong>Prompt reuse and project maturity</strong>: Reused or versioned prompts may indicate maturity and intentional design practices, potentially correlating with longevity or stability metrics.</p></li>
<li><p><strong>Temporal trends</strong>: Repositories created in 2023–2024 are expected to exhibit more sophisticated prompt structures, reflecting the broader adoption of LLM tooling and techniques during this period.</p></li>
</ul>
<p>Overall, I expect that prompt characteristics will serve as useful signals of project complexity, collaboration, and developer engagement—validating their role as first-class engineering artifacts.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<p>Several constraints may affect the strength or interpretability of the final results:</p>
<ul>
<li><p><strong>Weak correlation in early results</strong>: The pilot analysis revealed only a modest relationship between keyword count and engagement, indicating that high-level keyword matching may not fully capture the nuance of LLM integration. This raises the possibility that even prompt-level analysis might yield subtle or nonlinear associations.</p></li>
<li><p><strong>GitHub API rate limits</strong>: Repository and contributor metadata are retrieved through the GitHub API, which imposes rate limits. This restricts the volume of data that can be gathered in a short time and may limit the diversity or completeness of the sample.</p></li>
<li><p><strong>Noise and ambiguity in prompt detection</strong>: Extracting prompts from source code using static parsing and regex involves heuristics that may yield false positives (non-prompts) or miss edge cases (multi-line prompts, dynamic constructions). This could introduce noise in the classification phase.</p></li>
<li><p><strong>Resource-intensive processing</strong>: Extracting, deduplicating, and classifying prompts across hundreds of repositories is computationally expensive and time-consuming, especially when maintaining file- and commit-level traceability.</p></li>
<li><p><strong>Repository heterogeneity</strong>: Engagement metrics (e.g.&nbsp;stars, forks) are only rough proxies for real developer activity or quality. They may be influenced by external factors like social visibility, not just engineering practices.</p></li>
<li><p><strong>Limited generalizability</strong>: The project is currently scoped to Python repositories matching specific LLM-related keywords. As a result, findings may not generalize to other languages or non-GitHub ecosystems.</p></li>
</ul>
<p>Despite these limitations, the combination of pilot insights and structured prompt analysis provides a strong basis for exploring meaningful correlations between prompt engineering and real-world project dynamics.</p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Tafreshipour, A., Salami, M., &amp; Pradel, M. (2024). <em>Prompting in the Wild: An Empirical Study of Prompt Evolution in Software Repositories</em>. <em>arXiv preprint</em> <a href="https://arxiv.org/abs/2412.17298">arXiv:2412.17298</a></p>
<p>Zhou, W., Liu, J., &amp; Sun, M. (2024). <em>A Systematic Survey of Prompt Engineering in Large Language Models</em>. <em>arXiv preprint</em> <a href="https://arxiv.org/abs/2402.07927">arXiv:2402.07927</a></p>
<p>Li, X., Zhang, S., &amp; Chen, M. (2023). <em>ROSCOE: A Benchmark for Reasoning Over Chain-of-Thought Explanations</em>. <em>arXiv preprint</em> <a href="https://arxiv.org/abs/2212.07919">arXiv:2212.07919</a></p>
<p>Diao, W., Liu, Y., &amp; Su, Y. (2024). <em>Economical Prompting Index: Balancing Accuracy and Cost in LLM Prompts</em>. <em>arXiv preprint</em> <a href="https://arxiv.org/abs/2412.01690">arXiv:2412.01690</a></p>
<p><span class="math display">\[
\sim\sim\sim
\]</span></p>
</section>
<section id="disclaimer-use-of-ai" class="level2">
<h2 class="anchored" data-anchor-id="disclaimer-use-of-ai"><em>Disclaimer: Use of AI</em></h2>
<p>I am a full-blown AI enthusiast and believe it is as or more important for humanity than the printing press. I use it for absolutely everything.</p>
<section id="formal-disclaimer" class="level3">
<h3 class="anchored" data-anchor-id="formal-disclaimer"><em>Formal Disclaimer</em></h3>
<p>Portions of this proposal (initial topic brainstorming, paper identification, text drafting, coding, this paragraph) were generated with the assistance of OpenAI’s ChatGPT-4o. All content was critically reviewed, edited, and verified by the author. The model was used to drastically enhance productivity and clarity but not to replace original decision-making.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>