# SENG-404 Project Proposal: Prompt Engineering and Project Engagement in LLM-Enabled Repositories

Evan Strasdin - V00907185  
[Git Link](https://github.com/n4m3name/SENG404-Project)

## Motivation

Large Language Models (LLMs) like GPT-4 are now central to many software systems, with open-source tools such as LangChain, AutoGPT, and llama.cpp enabling developers to build entire applications around model behavior. In these systems, prompts are not just inputs: they function as logic, guiding how the model reasons, responds, and performs tasks. As a result, prompt engineering has become a core software development activity.

Recent research has begun to formalize this space. Studies have introduced taxonomies of prompt types, such as zero-shot, few-shot, and chain-of-thought prompting [(Zhou et al., 2024)](#references), and proposed metrics for evaluating prompt quality, including reasoning coherence [(Li et al., 2023)](#references) and cost-efficiency [(Diao et al., 2024)](#references). Other work has analyzed how prompts evolve over time in open-source repositories [(Tafreshipour et al., 2024)](#references).

However, it remains unclear whether prompt characteristics, both in terms of type and engineering behaviors (such as complexity, reuse, or collaborative authorship), are associated with broader indicators of project activity and engagement, such as contributor count, issue resolution rate, or project growth.

This study addresses that gap by combining prompt classification with software repository mining. We will apply established prompt type categories and measure engineering-related characteristics such as length, example count, and contributor overlap. Where appropriate, we may explore additional quality metrics inspired by recent research. Our goal is to uncover whether observable prompt patterns relate to collaboration and development dynamics in real-world LLM-integrated projects.

## Research Question

Do prompt characteristics, such as type, complexity, reuse, and collaborative authorship, correlate with indicators of project activity and engagement in open-source LLM-integrated repositories?

We will analyze prompt artifacts extracted from public repositories and apply a combination of classification (for example, by prompt type) and metric-based evaluation (such as prompt length and contributor interaction). These features will be compared against repository-level signals of activity, including contributor count, issue resolution rate, and project growth over time.

## Expected Data Sources

### Repository Selection (GitHub Search and GitHub API)  
To identify candidate repositories for our study, we performed a keyword-based search over public GitHub code using the [GitHub Search API](https://docs.github.com/en/rest/search?apiVersion=2022-11-28). We designed a list of 30+ LLM-related keywords targeting common APIs, prompt engineering patterns, and framework imports, such as:

* `openai.ChatCompletion.create`, `PromptTemplate`, `RetrievalQA`
* `"prompt = "`, `"system": "`
* `llama_cpp`, `AutoGPT`, `FAISS`, `transformers.pipeline`, and others

For each keyword, we retrieved up to 250 results across 5 paginated requests (50 results per page). Each code match was associated with metadata including the repository name, file path, and matched keyword.

To rank repository relevance, we grouped results by repository and counted how many distinct keywords each repository matched. The rationale is that repositories containing multiple LLM-specific keywords are more likely to integrate structured prompt engineering practices rather than incidental mentions.

The output is saved in both `.csv` and `.json` formats under the `/data` directory:

* [matched_repositories.csv](../data/matched_repositories.csv)
* [matched_repositories.json](../data/matched_repositories.json)

The complete code for this data collection and early processing process is documented in [data_sourcing.ipynb](../src/data_sourcing.ipynb).

### Project Metadata (GitHub API and GHTorrent)

To supplement our keyword-based prompt discovery with repository-level context, we retrieved metadata for each matched repository using the GitHub API.

For each repository, we collected:

* **Stars, forks, watchers** — as high-level proxies for community interest
* **Open issues** — to approximate project activity and maintenance
* **Repository timestamps** (`created_at`, `updated_at`, `pushed_at`) — for lifecycle analysis
* **License, language, and description** — to support filtering and categorization
* **Default branch** — required for linking to prompt files and future mining

This metadata was merged with our keyword-matching dataset (see [combined_repository_data.csv](../data/combined_repository_data.csv)), creating a unified view of prompt relevance and project health. The merged data is saved in both `.csv` and `.json` format to support both manual review and downstream analysis.

If time and scope allow, we may also incorporate contributor-level metrics (e.g. commit count, contributor overlap) and validate repository activity using [GHTorrent](http://ghtorrent.org/) to support scalable analysis beyond API rate limits.

### Prompt Artifacts  
Prompt strings will be extracted from relevant source files using a combination of static code parsing (e.g. Python AST), regex-based string analysis, and heuristics for identifying natural language input patterns. Prompts will be categorized by type using existing taxonomies [(Zhou et al., 2024)](#references), and features such as length, example count, and reuse will be computed automatically. File-level and commit-level metadata will be retained for all prompt artifacts.

### Optional: Software Heritage Graph  
If time and tooling permit, the Software Heritage Graph will be used to trace prompt files across forks and versions, allowing us to observe reuse and evolution beyond a single repository.

### Preliminary Validation: Pilot Analysis

To assess the reliability and relevance of our keyword-based repository selection, we conducted a series of exploratory analyses on the merged dataset, described in [../../src/pilot/pilot.qmd](https://github.com/n4m3name/SENG404-Project/blob/master/src/pilot/pilot.pdf). These pilot experiments served to validate the feasibility of our research design and guide the selection of downstream analysis metrics.

#### Key Findings:

* **Keyword Match Signal**: While a majority of repositories matched only a single LLM-related keyword, a substantial number exhibited 2+ matches—suggesting deeper integration of prompt-related logic. We use keyword count as a proxy for LLM integration intensity.
* **Engagement Metrics**: Repository engagement (stars, forks, watchers) showed wide variance across the sample, and log-transformed engagement distributions were suitable for analysis. This supports the use of these metrics as comparative signals for relevance and community interest.
* **Weak Correlation Observed**: A preliminary analysis found a weak positive correlation between the number of LLM-related keywords and a composite engagement score (Pearson’s r ≈ 0.07). This suggests that while there may not be a linear relationship, deeper patterns could emerge with prompt-level analysis.
* **Temporal Coverage**: The majority of repositories were created after 2020, with a sharp increase in 2023–2024, coinciding with the surge in public interest in LLMs. This confirms that our sample reflects contemporary LLM development practices.
* **Keyword Patterns**: Certain keywords—such as `"prompt="`, `"system"`, `ChatOpenAI`, and `PromptTemplate`—appeared with high frequency, and some showed stronger presence in high-engagement repositories. These patterns can inform our prompt classification strategy.

These initial results confirm that our keyword-matching and metadata pipeline successfully surfaces a diverse set of LLM-relevant repositories, suitable for further analysis. The pilot also helped us calibrate the importance of each metadata feature, refine our engagement scoring strategy, and identify prompts for extraction.



---

## Disclaimer: Use of AI
I am a full-blown AI enthusiast and believe it is as or more important for humanity than the printing press. I use it for absolutely everything, thus in order to conform to the the [suggested guidelines](https://llm-guidelines.org/guidelines/), I will link directly to the conversation(s) pertaining to the assignment.

### Formal Disclaimer
Portions of this proposal (initial topic brainstorming, paper identification, text drafts, this paragraph) were generated with the assistance of OpenAI's ChatGPT-4o. All content was critically reviewed, edited, and verified by the author. The model was used to enhance productivity and clarity but not to replace original analysis or decision-making.

[Link to Conversation](https://chatgpt.com/share/687ada87-7bf4-8001-8883-a3a9ed54e9f9)

---


## References

Tafreshipour, A., Salami, M., & Pradel, M. (2024). *Prompting in the Wild: An Empirical Study of Prompt Evolution in Software Repositories*. *arXiv preprint* [arXiv:2412.17298](https://arxiv.org/abs/2412.17298)

Zhou, W., Liu, J., & Sun, M. (2024). *A Systematic Survey of Prompt Engineering in Large Language Models*. *arXiv preprint* [arXiv:2402.07927](https://arxiv.org/abs/2402.07927)

Li, X., Zhang, S., & Chen, M. (2023). *ROSCOE: A Benchmark for Reasoning Over Chain-of-Thought Explanations*. *arXiv preprint* [arXiv:2212.07919](https://arxiv.org/abs/2212.07919)

Diao, W., Liu, Y., & Su, Y. (2024). *Economical Prompting Index: Balancing Accuracy and Cost in LLM Prompts*. *arXiv preprint* [arXiv:2412.01690](https://arxiv.org/abs/2412.01690)

